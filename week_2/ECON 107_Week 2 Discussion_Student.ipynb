{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "070811a8-1ce4-4fb2-be5b-8ca2db80c101",
   "metadata": {},
   "source": [
    "# Discussion 2 â€“ Regression Workflows with the Ames Housing Data\n",
    "\n",
    "## Welcome ðŸ‘‹\n",
    "\n",
    "In this notebook, you will:\n",
    "\n",
    "- Fit **linear regression** and understand **gradient descent**  \n",
    "- Explore **Ridge and Lasso regression** with feature transformations  \n",
    "- Perform **parameter tuning** and compare models using validation errors\n",
    "\n",
    "We will use a **reproducible workflow**: load and clean data, split into train/test, fit multiple models, and evaluate performance.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "By the end of this discussion, you will be able to:\n",
    "\n",
    "1. Clean and preprocess a real-world dataset  \n",
    "2. Implement **OLS regression** and **batch gradient descent**  \n",
    "3. Apply **Ridge and Lasso regression** with transformations  \n",
    "4. **Tune hyperparameters** and choose based on validation errors  \n",
    "5. **Compare models** using validation errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfd0bea-699d-478b-9954-e3b49c9d6a5e",
   "metadata": {},
   "source": [
    "## Step 1: Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8f224e-0b04-407e-bac1-01a3851eb331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor, Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from collections import Counter\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f78082-aa0f-48af-840d-aff9cca08335",
   "metadata": {},
   "source": [
    "## Step 2: Data Preperation\n",
    "- Load CSV data\n",
    "- Set random seed for reproducibility\n",
    "- Perform basic data cleaning (For demonstration purpose!! You should be more careful in this step for real-world application)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff496a9-a5e2-4247-9e45-cdbe9ea1aa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloaded from: https://www.kaggle.com/datasets/prevek18/ames-housing-dataset\n",
    "data = pd.read_csv(\"AmesHousing.csv\")\n",
    "\n",
    "# TODO: Set a random seed for reproducibility\n",
    "\n",
    "# Perform basic data cleaning for demonstration purpose\n",
    "selected_features = [\n",
    "    'Overall Qual',    # overall house quality rating\n",
    "    'Gr Liv Area',     # above-ground living area\n",
    "    'Total Bsmt SF',   # total basement area\n",
    "    'Garage Area',     # garage size\n",
    "    'Year Built',      # year house was built\n",
    "    'Full Bath',       # number of full bathrooms\n",
    "    'TotRms AbvGrd'    # total rooms above ground\n",
    "]\n",
    "\n",
    "data_clean = data[selected_features + [\"SalePrice\"]].copy()\n",
    "\n",
    "# Handle missing values by simple imputation (median)\n",
    "data_clean = data_clean.fillna(data_clean.median())\n",
    "\n",
    "# TODO: Normalize outcome through trimming and log transformation\n",
    "data_clean[\"SalePrice\"].hist()\n",
    "plt.show()\n",
    "\n",
    "### CODE START\n",
    "\n",
    "### CODE END\n",
    "\n",
    "data_clean[\"SalePrice\"].hist()\n",
    "plt.show()\n",
    "\n",
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8769c8-2bf8-4ee1-88a7-8237aecad7de",
   "metadata": {},
   "source": [
    "## Step 3: Train-Test Split & Scaling\n",
    "- Why train-test split?\n",
    "- Why scaling?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d659bf-3f77-4048-a8f1-6d087ba311e0",
   "metadata": {},
   "source": [
    "### 1. Train-Test Split\n",
    "- The test set acts as **unseen data** to evaluate how well our model generalizes.  \n",
    "- Prevents **overfitting**, since training and evaluation are on separate data.  \n",
    "- Standard workflow: train on `X_train`, tune parameters, evaluate on `X_test`.  \n",
    "\n",
    "### 2. Feature Scaling\n",
    "- Features with very different scales (e.g., `Gr Liv Area` vs `Full Bath`) can make **gradient descent slow or unstable**.  \n",
    "- Standardization puts all features on a **similar scale (mean 0, std 1)**.  \n",
    "- Important for **Ridge and Lasso**, which penalize coefficients directly.  \n",
    "- OLS can work without scaling, but scaling is recommended for gradient-based methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2566f598-2077-414c-a5cd-56a26e1f93b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_clean[selected_features]\n",
    "y = data_clean[\"SalePrice\"]\n",
    "\n",
    "# TODO: Split to 60/20/20 Train/Validation/Test \n",
    "\n",
    "print(\"Train size:\", X_train.shape)\n",
    "print(\"Validation size:\", X_val.shape)\n",
    "print(\"Test size:\", X_test.shape)\n",
    "\n",
    "# TODO: Perform Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = \n",
    "X_val_scaled = \n",
    "X_test_scaled = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a18ff9b-1192-4d93-8a44-fbbc9dbea125",
   "metadata": {},
   "source": [
    "## Step 4: Model Fitting\n",
    "\n",
    "### OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa35e545-2ed6-4a3a-9155-61e7014f2256",
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_model = LinearRegression()\n",
    "# TODO: Fit OLS\n",
    "ols_model.fit()\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = ols_model.predict(X_train_scaled)\n",
    "y_val_pred = ols_model.predict(X_val_scaled)\n",
    "y_test_pred = ols_model.predict(X_test_scaled)\n",
    "\n",
    "# Compute RMSE\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "print(f\"OLS Train RMSE: {train_rmse:.4f}\")\n",
    "print(f\"OLS Validation RMSE: {val_rmse:.4f}\")\n",
    "print(f\"OLS Test RMSE: {test_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab08ca4b-0c35-499b-afb2-ec176c377cff",
   "metadata": {},
   "source": [
    "### Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7310f29c-cb0b-4c94-bba0-7ece0b273e84",
   "metadata": {},
   "source": [
    "In this section, we implement **batch gradient descent** to estimate linear regression coefficients.\n",
    "\n",
    "#### Linear Regression Model\n",
    "\n",
    "We model the outcome as:\n",
    "\n",
    "$$\n",
    "\\hat{y} = X w\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $X \\in \\mathbb{R}^{n \\times (k+1)}$ is the design matrix (including an intercept column)  \n",
    "- $w \\in \\mathbb{R}^{k+1}$ is the vector of coefficients (including the intercept)  \n",
    "- $y \\in \\mathbb{R}^{n}$ is the outcome vector  \n",
    "- $n$ is the number of observations\n",
    "- $k$ is the number of features\n",
    "\n",
    "#### Cost Function\n",
    "\n",
    "We measure model fit using the **mean squared error (MSE)**:\n",
    "\n",
    "$$\n",
    "J(w) = \\frac{1}{2n} \\sum_{i=1}^{n} (x_i^\\top w - y_i)^2\n",
    "$$\n",
    "\n",
    "or equivalently in matrix form:\n",
    "\n",
    "$$\n",
    "J(w) = \\frac{1}{2n} \\| X w - y \\|^2\n",
    "$$\n",
    "\n",
    "- The factor $1/2$ simplifies the gradient computation  \n",
    "- Minimizing $J(w)$ is equivalent to ordinary least squares\n",
    "\n",
    "#### Gradient of the Cost Function\n",
    "\n",
    "After defining the cost function, the next step is to compute its **gradient** and use it to iteratively update the coefficients. \n",
    "\n",
    "The gradient of the mean squared error with respect to $w$ is:\n",
    "\n",
    "$$\n",
    "\\nabla_w J(w) = \\frac{1}{n} X^\\top (X w - y)\n",
    "$$\n",
    "\n",
    "#### Batch Gradient Descent\n",
    "\n",
    "The `gradient_descent` function puts everything together to **iteratively minimize the cost function** and find the best coefficients.\n",
    "\n",
    "At each iteration, the coefficients are updated using the gradient:\n",
    "\n",
    "$$\n",
    "w^{(t+1)} = w^{(t)} - \\alpha \\nabla_w J(w^{(t)})\n",
    "$$\n",
    "\n",
    "- $\\alpha$ = learning rate (controls step size)  \n",
    "- $\\nabla_w J(w^{(t)})$ = gradient of the cost at the current coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4d0925-b2da-4978-9d03-2f6a8ac23e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add intercept term to X matrices\n",
    "def add_intercept(X):\n",
    "    return np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "X_train_gd = add_intercept(X_train_scaled)\n",
    "X_val_gd   = add_intercept(X_val_scaled)\n",
    "X_test_gd  = add_intercept(X_test_scaled)\n",
    "\n",
    "y_train_vec = y_train.values.reshape(-1, 1)\n",
    "y_val_vec   = y_val.values.reshape(-1, 1)\n",
    "y_test_vec  = y_test.values.reshape(-1, 1)\n",
    "\n",
    "# ------------------------------\n",
    "# TODO: Cost function\n",
    "# ------------------------------\n",
    "def compute_cost(X, y, w):\n",
    "    n = X.shape[0]\n",
    "    residuals = \n",
    "    return \n",
    "\n",
    "# ------------------------------\n",
    "# TODO: Gradient\n",
    "# ------------------------------\n",
    "def compute_gradient(X, y, w):\n",
    "    n = X.shape[0]\n",
    "    residuals = \n",
    "    grad = \n",
    "    return grad\n",
    "\n",
    "# ------------------------------\n",
    "# TODO: Batch Gradient Descent\n",
    "# ------------------------------\n",
    "def gradient_descent(X, y, w_init, alpha, num_iters, tol=1e-10):\n",
    "    w = w_init.copy()\n",
    "    cost_history = []\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        ### CODE START\n",
    "\n",
    "        ## CODE END\n",
    "    return w, cost_history\n",
    "\n",
    "# ------------------------------\n",
    "# Run GD for multiple learning rates\n",
    "# ------------------------------\n",
    "n_features = X_train_gd.shape[1]\n",
    "w_init = np.zeros((n_features, 1))\n",
    "num_iters = 1000\n",
    "\n",
    "alphas = [0.01, 0.05, 0.1]\n",
    "gd_results = {}\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "\n",
    "for alpha in alphas:\n",
    "    w_gd, cost_history = gradient_descent(X_train_gd, y_train_vec, w_init, alpha, num_iters)\n",
    "    gd_results[alpha] = w_gd\n",
    "    plt.plot(range(len(cost_history)), cost_history, label=f\"alpha={alpha}\")\n",
    "\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost J(w)\")\n",
    "plt.title(\"Gradient Descent Convergence for Different Learning Rates (log-target)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------\n",
    "# GD weights to OLS\n",
    "# ------------------------------\n",
    "w_ols = np.r_[ols_model.intercept_, ols_model.coef_].reshape(-1, 1)\n",
    "\n",
    "for alpha, w_gd in gd_results.items():\n",
    "    diff = np.linalg.norm(w_gd - w_ols)\n",
    "    print(f\"|| w_GD - w_OLS || for alpha={alpha}: {diff:.10f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c032588b-cfc9-4075-8330-23b8af6ca086",
   "metadata": {},
   "source": [
    "### SGD, Ridge, and Lasso\n",
    "Use full feature for exercises below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c41aa95-f9dd-4a3f-a9a1-2fcbe1e016dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = data.columns.drop([\"Order\", \"PID\", \"SalePrice\"]).to_list()\n",
    "data_clean = data[all_features + [\"SalePrice\"]].copy()\n",
    "\n",
    "# Step 1: Drop columns with >50% missing values\n",
    "missing_pct = data_clean.isnull().mean()\n",
    "cols_to_keep = missing_pct[missing_pct <= 0.5].index\n",
    "data_clean = data_clean[cols_to_keep]\n",
    "\n",
    "# Step 2: Initial numeric and categorical separation\n",
    "numeric_features = data_clean.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "categorical_features = data_clean.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "# Step 3: Detect numeric-like categorical columns\n",
    "numeric_like_categorical = []\n",
    "\n",
    "for col in numeric_features:\n",
    "    num_unique = data_clean[col].nunique()\n",
    "    # Heuristic: if a numeric column has <15 unique values, it's likely categorical\n",
    "    if num_unique <= 15:\n",
    "        numeric_like_categorical.append(col)\n",
    "\n",
    "# Remove these from numeric_features and add to categorical_features\n",
    "numeric_features = [col for col in numeric_features if col not in numeric_like_categorical]\n",
    "categorical_features += numeric_like_categorical\n",
    "\n",
    "# Step 4: Impute missing values\n",
    "\n",
    "# Numeric: median\n",
    "data_clean[numeric_features] = data_clean[numeric_features].fillna(data_clean[numeric_features].median())\n",
    "\n",
    "# Categorical: mode\n",
    "for col in categorical_features:\n",
    "    mode_val = data_clean[col].mode()[0]\n",
    "    data_clean[col] = data_clean[col].fillna(mode_val)\n",
    "\n",
    "# Trim & log-transform SalePrice\n",
    "lower = data_clean.SalePrice.quantile(0.01)\n",
    "upper = data_clean.SalePrice.quantile(0.99)\n",
    "data_clean = data_clean[(data_clean.SalePrice >= lower) & (data_clean.SalePrice <= upper)]\n",
    "data_clean[\"SalePrice\"] = np.log(data_clean[\"SalePrice\"])\n",
    "\n",
    "X = data_clean.drop(columns = \"SalePrice\")\n",
    "y = data_clean[\"SalePrice\"]\n",
    "\n",
    "# Train/Validation/Test Split (60/20/20)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=random_seed\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=random_seed\n",
    ")\n",
    "\n",
    "numeric_features = [c for c in numeric_features if c in X_train.columns]\n",
    "categorical_features = [c for c in categorical_features if c in X_train.columns]\n",
    "\n",
    "# Preprocessing: Scaling + One-Hot Encoding\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    (\"num\", StandardScaler(), numeric_features),\n",
    "    (\"cat\", OneHotEncoder(sparse_output=False, handle_unknown='ignore'), categorical_features)\n",
    "])\n",
    "\n",
    "X_train_scaled = preprocessor.fit_transform(X_train)\n",
    "X_val_scaled = preprocessor.transform(X_val)\n",
    "X_test_scaled = preprocessor.transform(X_test)\n",
    "\n",
    "# Verify shapes\n",
    "print(\"Train shape:\", X_train_scaled.shape)\n",
    "print(\"Validation shape:\", X_val_scaled.shape)\n",
    "print(\"Test shape:\", X_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636d4d13-aa23-4bea-b5ec-46527c70c287",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb18777b-8695-4ad2-93cf-298885e2d384",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas_sgd = [1e-6, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "train_rmse_sgd = {}\n",
    "val_rmse_sgd = {}\n",
    "loss_curves = {}\n",
    "\n",
    "for alpha in alphas_sgd:\n",
    "    sgd = SGDRegressor(loss=\"squared_error\", eta0=alpha, max_iter=1, warm_start=True, random_state=random_seed)\n",
    "    n_epochs = 300\n",
    "    losses = []\n",
    "    \n",
    "    for _ in range(n_epochs):\n",
    "        sgd.fit(X_train_scaled, y_train)\n",
    "        preds = sgd.predict(X_train_scaled)\n",
    "        loss = mean_squared_error(y_train, preds)\n",
    "        losses.append(loss)\n",
    "    \n",
    "    loss_curves[alpha] = losses\n",
    "    train_rmse_sgd[alpha] = np.sqrt(mean_squared_error(y_train, sgd.predict(X_train_scaled)))\n",
    "    val_rmse_sgd[alpha] = np.sqrt(mean_squared_error(y_val, sgd.predict(X_val_scaled)))\n",
    "\n",
    "# Plot loss curves\n",
    "plt.figure(figsize=(9,6))\n",
    "for alpha, losses in loss_curves.items():\n",
    "    plt.plot(losses, label=f\"Î± = {alpha}\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.title(\"SGD: Cost vs Iteration\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "best_alpha_sgd = min(val_rmse_sgd, key=val_rmse_sgd.get)\n",
    "print(f\"Best SGD Î±: {best_alpha_sgd}\")\n",
    "print(f\"Train RMSE: {train_rmse_sgd[best_alpha_sgd]:.4f}\")\n",
    "print(f\"Validation RMSE: {val_rmse_sgd[best_alpha_sgd]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dc77f2-e688-40e0-a10a-4826bc306bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression (Full Feature)\n",
    "\n",
    "# TODO: Tune Lambda\n",
    "lambdas = np.logspace(-4, 4, 10)\n",
    "val_rmses = []\n",
    "best_ridge_rmse = float(\"inf\")\n",
    "best_ridge_alpha = None\n",
    "\n",
    "for alpha in lambdas:\n",
    "    ### CODE START\n",
    "\n",
    "    ### CODE END\n",
    "    \n",
    "    if rmse_val < best_ridge_rmse:\n",
    "        best_ridge_rmse = rmse_val\n",
    "        best_ridge_alpha = alpha\n",
    "        best_ridge_model = ridge\n",
    "\n",
    "print(f\"Best Ridge Î»: {best_ridge_alpha}\")\n",
    "print(f\"Train RMSE: {np.sqrt(mean_squared_error(y_train, best_ridge_model.predict(X_train_scaled))):.4f}\")\n",
    "print(f\"Validation RMSE: {best_ridge_rmse:.4f}\")\n",
    "\n",
    "# Plot Validation RMSE vs log(lambda)\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(np.log10(lambdas), val_rmses, marker='o', label='Validation RMSE')\n",
    "plt.axvline(np.log10(best_ridge_alpha), color='red', linestyle='--', label=f'Best Î» = {best_ridge_alpha:.4g}')\n",
    "plt.xlabel('log10(lambda)')\n",
    "plt.ylabel('Validation RMSE')\n",
    "plt.title('Ridge Regression: Validation RMSE vs log(lambda)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68127946-30eb-4fef-882f-4bee57213e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso Regression (Full Feature)\n",
    "\n",
    "# TODO: Tune Lambda\n",
    "best_lasso_rmse = float(\"inf\")\n",
    "best_lasso_alpha = None\n",
    "val_rmses_lasso = []\n",
    "\n",
    "for alpha in lambdas:\n",
    "    ### CODE START\n",
    "\n",
    "    ### CODE END\n",
    "    \n",
    "    if rmse_val < best_lasso_rmse:\n",
    "        best_lasso_rmse = rmse_val\n",
    "        best_lasso_alpha = alpha\n",
    "        best_lasso_model = lasso\n",
    "\n",
    "print(f\"Best Lasso Î»: {best_lasso_alpha}\")\n",
    "print(f\"Train RMSE: {np.sqrt(mean_squared_error(y_train, best_lasso_model.predict(X_train_scaled))):.4f}\")\n",
    "print(f\"Validation RMSE: {best_lasso_rmse:.4f}\")\n",
    "\n",
    "# Plot Validation RMSE vs log(lambda)\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(np.log10(lambdas), val_rmses_lasso, marker='o', label='Validation RMSE')\n",
    "plt.axvline(np.log10(best_lasso_alpha), color='red', linestyle='--', label=f'Best Î» = {best_lasso_alpha:.4g}')\n",
    "plt.xlabel('log10(lambda)')\n",
    "plt.ylabel('Validation RMSE')\n",
    "plt.title('Lasso Regression: Validation RMSE vs log(lambda)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ed80b6-0f4c-43b3-969a-54180303484d",
   "metadata": {},
   "source": [
    "### Polynomial Transformation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55152908-3fe2-406f-b279-cc276e9a9769",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_features = ['Gr Liv Area', 'Overall Qual']\n",
    "\n",
    "# TODO: Apply polynomial transformation\n",
    "poly = PolynomialFeatures()\n",
    "\n",
    "# Fit on selected features\n",
    "poly_matrix = poly.fit_transform(data_clean[poly_features])\n",
    "\n",
    "# Get new feature names\n",
    "poly_feature_names = poly.get_feature_names_out(poly_features)\n",
    "\n",
    "# Create DataFrame for new polynomial features (skip original features)\n",
    "poly_df = pd.DataFrame(poly_matrix[:, len(poly_features):], \n",
    "                       columns=poly_feature_names[len(poly_features):],\n",
    "                       index=data_clean.index)\n",
    "\n",
    "# Combine with original data_clean\n",
    "data_aug = pd.concat([data_clean, poly_df], axis=1)\n",
    "\n",
    "# Check shape and first few rows\n",
    "print(\"Original data_clean shape:\", data_clean.shape)\n",
    "print(\"Augmented data shape:\", data_aug.shape)\n",
    "data_aug.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c9d129-8107-42c0-8bf7-5ce4c3f29a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!! SAME PROCESS AS BEFORE, JUST WITH data_aug INSTEAD OF data_clean !!!\n",
    "X = data_aug.drop(columns=\"SalePrice\")\n",
    "y = data_aug[\"SalePrice\"]\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=random_seed\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=random_seed\n",
    ")\n",
    "\n",
    "numeric_features_aug = numeric_features + poly_df.columns.tolist()\n",
    "numeric_features_aug = [c for c in numeric_features_aug if c in X_train.columns]\n",
    "categorical_features_aug = [c for c in categorical_features if c in X_train.columns]\n",
    "\n",
    "preprocessor_aug = ColumnTransformer(transformers=[\n",
    "    (\"num\", StandardScaler(), numeric_features_aug),\n",
    "    (\"cat\", OneHotEncoder(sparse_output=False, handle_unknown='ignore'), categorical_features_aug)\n",
    "])\n",
    "\n",
    "X_train_scaled_aug = preprocessor_aug.fit_transform(X_train)\n",
    "X_val_scaled_aug = preprocessor_aug.transform(X_val)\n",
    "X_test_scaled_aug = preprocessor_aug.transform(X_test)\n",
    "\n",
    "print(\"Train shape (augmented):\", X_train_scaled_aug.shape)\n",
    "print(\"Validation shape (augmented):\", X_val_scaled_aug.shape)\n",
    "print(\"Test shape (augmented):\", X_test_scaled_aug.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7766e9c7-11af-4343-843d-85033b8b6b6e",
   "metadata": {},
   "source": [
    "### Ridge with Feature Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27d8e01-5e36-400d-9c21-f74ca69fe2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression on Augmented Features\n",
    "lambdas = np.logspace(-4, 4, 10)\n",
    "val_rmses_aug = []\n",
    "best_ridge_rmse_aug = float(\"inf\")\n",
    "best_ridge_alpha_aug = None\n",
    "\n",
    "for alpha in lambdas:\n",
    "    ridge = Ridge(alpha=alpha, random_state=random_seed)\n",
    "    ridge.fit(X_train_scaled_aug, y_train)\n",
    "    rmse_val = np.sqrt(mean_squared_error(y_val, ridge.predict(X_val_scaled_aug)))\n",
    "    val_rmses_aug.append(rmse_val)\n",
    "    \n",
    "    if rmse_val < best_ridge_rmse_aug:\n",
    "        best_ridge_rmse_aug = rmse_val\n",
    "        best_ridge_alpha_aug = alpha\n",
    "        best_ridge_model_aug = ridge\n",
    "\n",
    "print(f\"Best Ridge Î» (augmented): {best_ridge_alpha_aug}\")\n",
    "print(f\"Train RMSE: {np.sqrt(mean_squared_error(y_train, best_ridge_model_aug.predict(X_train_scaled_aug))):.4f}\")\n",
    "print(f\"Validation RMSE: {best_ridge_rmse_aug:.4f}\")\n",
    "\n",
    "# Plot Validation RMSE vs log(lambda)\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(np.log10(lambdas), val_rmses_aug, marker='o', label='Validation RMSE')\n",
    "plt.axvline(np.log10(best_ridge_alpha_aug), color='red', linestyle='--', \n",
    "            label=f'Best Î» = {best_ridge_alpha_aug:.4g}')\n",
    "plt.xlabel('log10(lambda)')\n",
    "plt.ylabel('Validation RMSE')\n",
    "plt.title('Ridge Regression (Augmented): Validation RMSE vs log(lambda)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33cb6a0-46c9-420d-9097-2472255128b3",
   "metadata": {},
   "source": [
    "### Lasso with Feature Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244f7162-d1b6-402c-a725-d23678aade11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso Regression on Augmented Features\n",
    "best_lasso_rmse_aug = float(\"inf\")\n",
    "best_lasso_alpha_aug = None\n",
    "val_rmses_lasso_aug = []\n",
    "\n",
    "for alpha in lambdas:\n",
    "    lasso = Lasso(alpha=alpha, random_state=random_seed)\n",
    "    lasso.fit(X_train_scaled_aug, y_train)\n",
    "    rmse_val = np.sqrt(mean_squared_error(y_val, lasso.predict(X_val_scaled_aug)))\n",
    "    val_rmses_lasso_aug.append(rmse_val)\n",
    "    \n",
    "    if rmse_val < best_lasso_rmse_aug:\n",
    "        best_lasso_rmse_aug = rmse_val\n",
    "        best_lasso_alpha_aug = alpha\n",
    "        best_lasso_model_aug = lasso\n",
    "\n",
    "print(f\"Best Lasso Î» (augmented): {best_lasso_alpha_aug}\")\n",
    "print(f\"Train RMSE: {np.sqrt(mean_squared_error(y_train, best_lasso_model_aug.predict(X_train_scaled_aug))):.4f}\")\n",
    "print(f\"Validation RMSE: {best_lasso_rmse_aug:.4f}\")\n",
    "\n",
    "# Plot Validation RMSE vs log(lambda)\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(np.log10(lambdas), val_rmses_lasso_aug, marker='o', label='Validation RMSE')\n",
    "plt.axvline(np.log10(best_lasso_alpha_aug), color='red', linestyle='--', \n",
    "            label=f'Best Î» = {best_lasso_alpha_aug:.4g}')\n",
    "plt.xlabel('log10(lambda)')\n",
    "plt.ylabel('Validation RMSE')\n",
    "plt.title('Lasso Regression (Augmented): Validation RMSE vs log(lambda)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5c8b85-8ca3-445d-bb77-221774ccc54c",
   "metadata": {},
   "source": [
    "### Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be8b90e-d1b2-4989-8a40-96489dbfeacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generate summary table\n",
    "rmse_summary = pd.DataFrame({\n",
    "    \"Model\": [\n",
    "        \"SGD Linear Regression\", \n",
    "        \"Ridge (Full Features)\", \n",
    "        \"Lasso (Full Features)\", \n",
    "        \"Ridge (Augmented Features)\", \n",
    "        \"Lasso (Augmented Features)\"\n",
    "    ],\n",
    "    \"Validation RMSE\": [\n",
    "        ### CODE START\n",
    "\n",
    "        ### CODE END\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Sort by validation RMSE\n",
    "rmse_summary = \n",
    "rmse_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a904c3-8d5b-47c8-8f85-737cfaee5664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate on test for best model\n",
    "best_model_name = \n",
    "print(f\"Best model based on validation RMSE: {best_model_name}\")\n",
    "print(f\"Test RMSE: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6783a2a7-672c-4479-9a90-753e8c71cb96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
