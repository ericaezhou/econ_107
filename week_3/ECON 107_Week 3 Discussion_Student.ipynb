{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3fa88fb-a6f3-4b59-beab-c44c0df46266",
   "metadata": {},
   "source": [
    "# Discussion 3 â€“ Classification Workflows with Logistic Regression\n",
    "\n",
    "## Welcome ðŸ‘‹\n",
    "\n",
    "In this notebook, you will:\n",
    "\n",
    "- Build intuition for **logistic regression** using small synthetic datasets\n",
    "- Understand **classification errors** and evaluation metrics\n",
    "- Use **ROC curves** and **AUC** to compare classifiers\n",
    "- Apply the full workflow to the **South German Credit** dataset\n",
    "\n",
    "We will first study each concept in isolation using simple, synthetic data. Then, we will combine everything into a complete real-world workflow, which will help a lot for **Assignment 2**!\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "By the end of this discussion, you will be able to:\n",
    "\n",
    "1. Implement logistic regression using **sklearn**\n",
    "3. Understand classification errors through **confusion matrices**\n",
    "4. Evaluate classifiers using **ROC** and **AUC**\n",
    "5. Apply a full classification **workflow** to real credit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20701635-2850-4dc9-b416-dd4ce7e7c9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, log_loss, roc_auc_score, accuracy_score, RocCurveDisplay\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "random_seed = 42\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fb6154-f9fc-413f-97d5-91de96d793c5",
   "metadata": {},
   "source": [
    "## Concept 1: Logistic Regression as a Probability Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f135eaa-e779-4aed-81b1-420ce930b4f8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Suppose we observe a single feature $x$ and a binary outcome $y$:\n",
    "\n",
    "- $x$: hours studied\n",
    "- $y = 1$: pass\n",
    "- $y = 0$: fail\n",
    "\n",
    "We want to model the probability of passing as a function of hours studied.\n",
    "\n",
    "Logistic regression models:\n",
    "\n",
    "$$\n",
    "P(y = 1 \\mid x) = \\sigma(w_0 + w_1 x)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "As $x$ increases:\n",
    "- $w_0 + w_1 x$ increases\n",
    "- the sigmoid output moves smoothly from 0 to 1\n",
    "- predictions can be interpreted as probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd81d0d-cb08-4e86-bf0e-20dc6754fe3a",
   "metadata": {},
   "source": [
    "### Why Not Linear Regression?\n",
    "\n",
    "If we used linear regression here, predictions could be:[-inf, inf]\n",
    "\n",
    "Logistic regression avoids this by constraining predictions to $[0,1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e785b44-a5cb-4d12-bbd3-35c2c3596065",
   "metadata": {},
   "source": [
    "## Concept 2: Logistic Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a3a480-a2a4-4cfc-8b7a-3a4896dcd79c",
   "metadata": {},
   "source": [
    "Suppose a student actually **passed**, so the true label is:\n",
    "\n",
    "$$\n",
    "y = 1\n",
    "$$\n",
    "\n",
    "The logistic loss for a single observation is:\n",
    "\n",
    "$$\n",
    "\\ell(y, \\hat{p})\n",
    "= - \\left[ y \\log(\\hat{p}) + (1 - y)\\log(1 - \\hat{p}) \\right]\n",
    "$$\n",
    "\n",
    "Because $y = 1$, this simplifies to:\n",
    "\n",
    "$$\n",
    "\\ell(1, \\hat{p}) = -\\log(\\hat{p})\n",
    "$$\n",
    "\n",
    "\n",
    "Suppose the model predicts:\n",
    "\n",
    "$$\n",
    "\\hat{p} = 0.9\n",
    "$$\n",
    "\n",
    "The loss is:\n",
    "\n",
    "$$\n",
    "\\ell(1, 0.9) = -\\log(0.9) \\approx 0.105\n",
    "$$\n",
    "\n",
    "Now suppose the model predicts:\n",
    "\n",
    "$$\n",
    "\\hat{p} = 0.1\n",
    "$$\n",
    "\n",
    "Then the loss is:\n",
    "\n",
    "$$\n",
    "\\ell(1, 0.1) = -\\log(0.1) \\approx 2.303\n",
    "$$\n",
    "\n",
    "This loss is **more than 20Ã— larger** than in the previous case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b61266-0c2d-4130-9062-25c56140fdda",
   "metadata": {},
   "source": [
    "## Concept 3: Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a7765d-a3da-4f04-bf6a-fcf91a7ebce5",
   "metadata": {},
   "source": [
    "Consider a toy dataset with one feature:\n",
    "\n",
    "- $x = [1, 2, 3]$\n",
    "- $y = [0, 0, 1]$\n",
    "\n",
    "Suppose the model initially predicts high probabilities for all points.\n",
    "\n",
    "Gradient descent works by:\n",
    "1. Computing predicted probabilities\n",
    "2. Measuring how wrong those predictions are\n",
    "3. Updating parameters to reduce future errors\n",
    "\n",
    "If the model predicts a high probability for $y=0$ points, the gradient pushes parameters to lower those probabilities. Iteration by iteration, the decision boundary shifts until predicted probabilities align with observed labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be74677e-394b-4541-836d-da6dac895042",
   "metadata": {},
   "source": [
    "## Concept 4: From Probabilities to Class Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad413ba-ee72-4dc2-ab18-1325354c4197",
   "metadata": {},
   "source": [
    "Suppose a model predicts probabilities:\n",
    "\n",
    "| Observation | $\\hat{p}$ |\n",
    "|-------------|-----------|\n",
    "| A           | 0.90      |\n",
    "| B           | 0.60      |\n",
    "| C           | 0.40      |\n",
    "| D           | 0.10      |\n",
    "\n",
    "With threshold $\\tau = 0.5$:\n",
    "- A, B â†’ class 1\n",
    "- C, D â†’ class 0\n",
    "\n",
    "With threshold $\\tau = 0.7$:\n",
    "- A â†’ class 1\n",
    "- B, C, D â†’ class 0\n",
    "\n",
    "Changing the threshold changes **who gets classified as positive**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e7f814-b563-4b12-b1a3-741af810f970",
   "metadata": {},
   "source": [
    "## Concept 5: Confusion Matrix and Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6987002-8276-4b9b-b991-78e8b5fb19e8",
   "metadata": {},
   "source": [
    "### The Confusion Matrix\n",
    "\n",
    "For a binary classifier, predictions can be summarized in a **confusion matrix**.\n",
    "\n",
    "|                    | Predicted Positive | Predicted Negative |\n",
    "|--------------------|-------------------|-------------------|\n",
    "| **Actual Positive** | True Positive (TP) | False Negative (FN) |\n",
    "| **Actual Negative** | False Positive (FP) | True Negative (TN) |\n",
    "\n",
    "In words:\n",
    "- **TP**: correctly predict positive\n",
    "- **FP**: predict positive when the true label is negative\n",
    "- **TN**: correctly predict negative\n",
    "- **FN**: predict negative when the true label is positive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8afad42-2fdf-4cc1-b9ba-9da36f98e0d6",
   "metadata": {},
   "source": [
    "### Error Metrics Derived from the Confusion Matrix\n",
    "\n",
    "From these four quantities, we can define common evaluation metrics.\n",
    "\n",
    "**True Positive Rate (TPR)**, also called *recall* or *sensitivity*:\n",
    "\n",
    "$$\n",
    "\\text{TPR} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n",
    "$$\n",
    "\n",
    "This answers:\n",
    "> Of all actual positives, how many did we correctly identify?\n",
    "\n",
    "**False Positive Rate (FPR)**:\n",
    "\n",
    "$$\n",
    "\\text{FPR} = \\frac{\\text{FP}}{\\text{FP} + \\text{TN}}\n",
    "$$\n",
    "\n",
    "This answers:\n",
    "> Of all actual negatives, how many did we incorrectly label as positive?\n",
    "\n",
    "**Accuracy**:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}\n",
    "{\\text{TP} + \\text{FP} + \\text{TN} + \\text{FN}}\n",
    "$$\n",
    "\n",
    "This answers:\n",
    "> What fraction of predictions were correct?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82e545e-a89b-4bc6-ba76-db4d116caf06",
   "metadata": {},
   "source": [
    "### Why These Metrics Can Be Misleading?\n",
    "\n",
    "At first glance, metrics like accuracy, TPR, and FPR seem reasonable. However, they can be **deeply misleading** when the data are imbalanced.\n",
    "\n",
    "Suppose we are predicting rare events:\n",
    "\n",
    "- 1000 observations total\n",
    "- 950 negatives\n",
    "- 50 positives (only 5% positive class)\n",
    "\n",
    "Consider a trivial classifier that **always predicts negative**, then:\n",
    "\n",
    "|                    | Predicted Positive | Predicted Negative |\n",
    "|--------------------|-------------------|-------------------|\n",
    "| **Actual Positive** | TP = 0             | FN = 50            |\n",
    "| **Actual Negative** | FP = 0             | TN = 950           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed86a74-9fb2-493e-a2a3-7dba5b2c4e76",
   "metadata": {},
   "source": [
    "**Accuracy**\n",
    "\n",
    "$$\n",
    "\\text{Accuracy}\n",
    "= \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{FP} + \\text{TN} + \\text{FN}}\n",
    "= \\frac{950}{1000}\n",
    "= 95\\%\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d839bf42-d57e-47eb-bc24-4feafb7eb13f",
   "metadata": {},
   "source": [
    "**True Positive Rate (TPR)**\n",
    "\n",
    "$$\n",
    "\\text{TPR}\n",
    "= \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n",
    "= \\frac{0}{50}\n",
    "= 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a72c05-ab65-4c57-a01c-0ae68a202bac",
   "metadata": {},
   "source": [
    "**False Positive Rate (FPR)**\n",
    "\n",
    "$$\n",
    "\\text{FPR}\n",
    "= \\frac{\\text{FP}}{\\text{FP} + \\text{TN}}\n",
    "= \\frac{0}{950}\n",
    "= 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad41c08-1e7d-468f-aa74-e06eb9846c5e",
   "metadata": {},
   "source": [
    "At first glance, the model might appear to perform well **(high accuracy, low FPR)**. But in reality, it's **HORRIBLE**, useless in detecting the thing we care about."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c7dbbd-96ba-44e2-a567-4a5588e493a6",
   "metadata": {},
   "source": [
    "### Class Imbalance and Threshold Dependence\n",
    "\n",
    "This example highlights two major issues:\n",
    "\n",
    "1. **Class imbalance**: When one class dominates, accuracy can be high even for bad models\n",
    "\n",
    "2. **Threshold dependence**: TP, FP, TN, FN all depend on a chosen decision threshold\n",
    "\n",
    "ROC curves solve these problems by evaluating classifier performance **across all possible thresholds**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376ec940-a09f-4c8f-9684-65e5146d1447",
   "metadata": {},
   "source": [
    "## Concept 6: ROC and AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52969571-b9f4-4083-a794-3b816ff3ba63",
   "metadata": {},
   "source": [
    "**Key components:**\n",
    "- **X-axis**: False Positive Rate (FPR) = FP / (FP + TN) = 1 - Specificity\n",
    "- **Y-axis**: True Positive Rate (TPR) = TP / (TP + FN) = Sensitivity = Recall\n",
    "\n",
    "**Why is this useful?**\n",
    "- Shows the **trade-off** between catching more positives (TPR) and accepting more false alarms (FPR)\n",
    "- Evaluates performance across **all** thresholds, not just one\n",
    "\n",
    "**To build an ROC curve:**\n",
    "1. Vary the decision threshold from 1 to 0\n",
    "2. Compute a confusion matrix at each threshold\n",
    "3. Calculate TPR and FPR from each confusion matrix\n",
    "4. Plot TPR vs FPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7080567-a9d0-4482-904a-61e79a0f9b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Step 0: Setup and train model\n",
    "# -------------------------\n",
    "np.random.seed(42)\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=1000, n_features=20, n_informative=15, \n",
    "    n_redundant=5, n_classes=2, random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "lr_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_proba = lr_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "print(\"Sample predicted probabilities:\", y_proba[:10])\n",
    "print(\"Corresponding true labels:   \", y_test[:10])\n",
    "\n",
    "# -------------------------\n",
    "# Step 1: Define thresholds and compute TPR/FPR\n",
    "# -------------------------\n",
    "thresholds = [0.9, 0.7, 0.5, 0.3, 0.1]\n",
    "\n",
    "tpr_list = []\n",
    "fpr_list = []\n",
    "results_data = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred = (y_proba >= threshold).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    \n",
    "    tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    \n",
    "    tpr_list.append(tpr)\n",
    "    fpr_list.append(fpr)\n",
    "    \n",
    "    results_data.append({\n",
    "        'Threshold': threshold,\n",
    "        'TPR': tpr,\n",
    "        'FPR': fpr,\n",
    "        'TP': tp,\n",
    "        'FP': fp,\n",
    "        'TN': tn,\n",
    "        'FN': fn\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results_data)\n",
    "print(df_results.to_string(index=False))\n",
    "\n",
    "# -------------------------\n",
    "# Step 2: Plot ROC points\n",
    "# -------------------------\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(fpr_list, tpr_list, s=100, c=thresholds, cmap='viridis', \n",
    "            edgecolors='black', linewidths=2, zorder=3)\n",
    "plt.plot(fpr_list, tpr_list, 'b--', alpha=0.5, label='ROC points')\n",
    "plt.plot([0, 1], [0, 1], 'r--', label='Random Classifier', linewidth=2)\n",
    "\n",
    "for i, threshold in enumerate(thresholds):\n",
    "    plt.annotate(f't={threshold}', (fpr_list[i], tpr_list[i]), \n",
    "                 xytext=(10, -5), textcoords='offset points', fontsize=9)\n",
    "\n",
    "plt.xlabel('False Positive Rate (FPR)', fontsize=12)\n",
    "plt.ylabel('True Positive Rate (TPR)', fontsize=12)\n",
    "plt.title('Step-by-Step ROC Construction (Manual Thresholds)', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8b2d47-e111-4442-881a-11537ccaacf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# ROC and AUC Pipeline\n",
    "# -------------------------\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=3, \n",
    "         label=f'Logistic Regression (AUC = {roc_auc:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', \n",
    "         label='Random Classifier (AUC = 0.500)')\n",
    "\n",
    "default_idx = np.argmin(np.abs(thresholds - 0.5))\n",
    "plt.scatter(fpr[default_idx], tpr[default_idx], s=200, c='red', \n",
    "            marker='*', edgecolors='black', linewidths=2, zorder=5,\n",
    "            label=f'Default threshold = 0.5')\n",
    "\n",
    "plt.xlabel('False Positive Rate', fontsize=13)\n",
    "plt.ylabel('True Positive Rate', fontsize=13)\n",
    "plt.title('ROC Curve: Logistic Regression', fontsize=15, fontweight='bold')\n",
    "plt.legend(loc=\"lower right\", fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9de3f8-ae6d-43a9-93c2-3b1920c072fc",
   "metadata": {},
   "source": [
    "The ROC curve visualizes the **trade-off** between TPR and FPR across all thresholds. Each point represents a threshold choice, showing how being more conservative or aggressive affects predictions.\n",
    "\n",
    "- The **default threshold (0.5)** is often a balanced starting point.  \n",
    "- For **high-risk applications** where missing positives is costly (e.g., fraud detection), we can **lower the threshold** to increase TPR, accepting more false positives.  \n",
    "- For **cost-sensitive scenarios** where false positives are costly (e.g., sending notifications), we can **raise the threshold** to reduce FPR.  \n",
    "\n",
    "A common heuristic is to **choose the point closest to the upper-left corner**, which maximizes TPR while minimizing FPR. The ROC curve helps **visually identify this optimal trade-off** based on context and business priorities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07088370-de57-4497-b703-13a4b80d9e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Step 0: Define and train multiple models\n",
    "# -------------------------\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Decision Tree (depth=5)': DecisionTreeClassifier(random_state=42, max_depth=5),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Weak Tree (depth=1)': DecisionTreeClassifier(random_state=42, max_depth=1)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    else:\n",
    "        y_proba = model.decision_function(X_test_scaled)\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    results[name] = {'fpr': fpr, 'tpr': tpr, 'auc': roc_auc}\n",
    "\n",
    "# -------------------------\n",
    "# Step 1: Rank models by AUC\n",
    "# -------------------------\n",
    "ranked_models = sorted(results.items(), key=lambda x: x[1]['auc'], reverse=True)\n",
    "\n",
    "print(\"Models ranked by AUC:\")\n",
    "for i, (name, result) in enumerate(ranked_models, 1):\n",
    "    print(f\"{i}. {name:<25s} AUC = {result['auc']:.4f}\")\n",
    "\n",
    "# -------------------------\n",
    "# Step 2: Plot ROC curves for all models\n",
    "# -------------------------\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = ['blue', 'green', 'red', 'purple', 'orange']\n",
    "\n",
    "for (name, result), color in zip(results.items(), colors):\n",
    "    plt.plot(result['fpr'], result['tpr'], lw=2.5, color=color,\n",
    "             label=f\"{name} (AUC = {result['auc']:.3f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--', \n",
    "         label='Random Classifier (AUC = 0.500)', alpha=0.7)\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=13)\n",
    "plt.ylabel('True Positive Rate', fontsize=13)\n",
    "plt.title('ROC Curve Comparison: Multiple Models', fontsize=15, fontweight='bold')\n",
    "plt.legend(loc=\"lower right\", fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4558189-f566-46c8-8b1d-c577eb0d314f",
   "metadata": {},
   "source": [
    "The Area Under the ROC Curve (AUC) provides a single-number summary of a modelâ€™s ability to balance TPR and FPR across all thresholds. By plotting multiple models on the same graph, we can compare their overall classification performance.\n",
    "\n",
    "- Models with curves that **bow closer to the upper-left corner** have higher AUC, meaning they generally achieve higher TPR for lower FPR.  \n",
    "- The **Weak Tree (depth=1)** has a curve close to the diagonal and low AUC, indicating poor classification ability.  \n",
    "- **Random Forest** typically has the highest AUC due to its ensemble structure, producing a curve that stays high and to the left.  \n",
    "- All models are above the diagonal line, meaning they perform better than random guessing.\n",
    "\n",
    "A larger AUC generally indicates **better average performance across all thresholds**, but it does **not guarantee the model is absolutely better** in every scenario:\n",
    "\n",
    "1. **Context matters:** Some applications prioritize **high recall** (e.g., detecting fraud) or **low false positives** (e.g., costly notifications). A model with slightly lower AUC might be preferred if it performs better in the relevant threshold region.  \n",
    "2. **Threshold-specific trade-offs:** AUC averages over all thresholds, but in practice we use a **specific threshold**. Two models with similar AUC could behave very differently at the threshold we actually choose.  \n",
    "3. **Other factors:** Simplicity, interpretability, training time, and robustness might make a model with lower AUC more suitable in certain contexts.\n",
    "\n",
    "**Practical takeaway:** Use AUC to **screen and rank models**, but combine it with **threshold analysis and domain priorities** to select the best model for your specific problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebceb69-4913-441b-bd2e-d1e586c0162d",
   "metadata": {},
   "source": [
    "## Real Data Example: South German Credit\n",
    "\n",
    "Each row represents one applicant, and the target variable indicates whether the applicant was a **good credit risk** (y = 1)or a **bad credit risk** (y = 0). Our goal is to **predict the probability that an applicant is a good credit risk**, based on their observed characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc794c26-3d67-480d-bf0d-147327e7aeb8",
   "metadata": {},
   "source": [
    "## Part 1: Data Exploration and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26236a0-a85e-4872-a509-880190dad379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable names from UCI documentation\n",
    "colnames = [\n",
    "    \"status\",                 # laufkont\n",
    "    \"duration\",               # laufzeit\n",
    "    \"credit_history\",         # moral\n",
    "    \"purpose\",                # verw\n",
    "    \"amount\",                 # hoehe\n",
    "    \"savings\",                # sparkont\n",
    "    \"employment_duration\",    # beszeit\n",
    "    \"installment_rate\",       # rate\n",
    "    \"personal_status_sex\",    # famges\n",
    "    \"other_debtors\",          # buerge\n",
    "    \"present_residence\",      # wohnzeit\n",
    "    \"property\",               # verm\n",
    "    \"age\",                    # alter\n",
    "    \"other_installment_plans\",# weitkred\n",
    "    \"housing\",                # wohn\n",
    "    \"number_credits\",         # bishkred\n",
    "    \"job\",                    # beruf\n",
    "    \"people_liable\",          # pers\n",
    "    \"telephone\",              # telef\n",
    "    \"foreign_worker\",         # gastarb\n",
    "    \"credit_risk\"             # kredit\n",
    "]\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"SouthGermanCredit.asc\",\n",
    "    sep=r\"\\s+\",\n",
    "    names=colnames,\n",
    "    engine=\"python\",\n",
    "    header=0\n",
    ")\n",
    "\n",
    "y = df[\"credit_risk\"]\n",
    "X = df.drop(columns=\"credit_risk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e6a353-9f52-4b2d-afef-cd8e7d07e06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of observations and predictors\n",
    "print(\"Number of observations:\", X.shape[0])\n",
    "print(\"Number of predictors:\", X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5973388a-0b59-437e-abf4-81be37275488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use documentation to identify categorical versus numeric features\n",
    "# https://archive.ics.uci.edu/dataset/522/south+german+credit\n",
    "categorical_features = [\n",
    "    \"status\",\n",
    "    \"credit_history\",\n",
    "    \"purpose\",\n",
    "    \"savings\",\n",
    "    \"personal_status_sex\",\n",
    "    \"other_debtors\",\n",
    "    \"other_installment_plans\",\n",
    "    \"housing\"\n",
    "]\n",
    "\n",
    "numeric_features = [\n",
    "    \"duration\",\n",
    "    \"amount\",\n",
    "    \"employment_duration\",\n",
    "    \"installment_rate\",\n",
    "    \"present_residence\",\n",
    "    \"property\",\n",
    "    \"age\",\n",
    "    \"number_credits\",\n",
    "    \"job\",\n",
    "    \"people_liable\",\n",
    "    \"telephone\",\n",
    "    \"foreign_worker\"\n",
    "]\n",
    "\n",
    "print(\"Categorical predictors:\", categorical_features)\n",
    "\n",
    "print(\"\\nNumeric predictors:\", numeric_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11532f4d-5907-4aa1-8398-91bc3d3ef4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values\n",
    "print(\"Missing values by column:\")\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add91e80-c559-4bdc-8255-dd95114aa322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding for categorical\n",
    "# naive way of handeling missing values: numeric -> median; categorical -> mode\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5c2dc2-476e-4000-845a-b821a5692bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: proportion of positive outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac10cee-2a0b-473a-a78c-796a680ce8b7",
   "metadata": {},
   "source": [
    "## Part 2: Train, Validation, and Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23bb6e9-c36e-4385-ac5f-765efb8a04bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: split train test with stratification\n",
    "X_train, X_temp, y_train, y_temp = train_test_split()\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split()\n",
    "\n",
    "print(\"Pr(y = 1) in training set:\", np.mean(y_train))\n",
    "print(\"Pr(y = 1) in validation set:\", np.mean(y_val))\n",
    "print(\"Pr(y = 1) in test set:\", np.mean(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f660d43-aa56-496b-9359-4ceb3241336b",
   "metadata": {},
   "source": [
    "### What happens when you DON'T stratify?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cd7a90-d884-49a7-bbaa-883d9ac365ff",
   "metadata": {},
   "source": [
    "## Part 3: Logistic Regression with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059500d6-6892-467a-afca-c5c6d3cf834a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build baseline logistic regression WITHOUT regularization\n",
    "baseline_model = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"logreg\", LogisticRegression(# fill in hyperparameters))])\n",
    "\n",
    "# Tune the regularization parameter C\n",
    "C_grid = np.logspace(-3, 3, 7)\n",
    "\n",
    "val_log_loss = []\n",
    "val_auc = []\n",
    "\n",
    "# For each C, compute and store: validation log loss, validation AUC\n",
    "for C in C_grid:\n",
    "    model = Pipeline(steps=[\n",
    "        (\"preprocess\", preprocess),\n",
    "        (\"logreg\", LogisticRegression(\n",
    "            C=C,\n",
    "            solver=\"lbfgs\",\n",
    "            max_iter=1000,\n",
    "            random_state=random_seed\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_val_prob = model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    val_log_loss.append(log_loss(y_val, y_val_prob))\n",
    "    val_auc.append(roc_auc_score(y_val, y_val_prob))\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    \"C\": C_grid,\n",
    "    \"lambda\": 1 / C_grid,\n",
    "    \"log_C\": np.log10(C_grid),\n",
    "    \"val_log_loss\": val_log_loss,\n",
    "    \"val_auc\": val_auc\n",
    "})\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeee106b-53f5-4a08-b710-7352d7badf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot validation log loss versus log(C)\n",
    "plt.figure()\n",
    "plt.plot(results[\"log_C\"], results[\"val_log_loss\"], marker=\"o\")\n",
    "plt.xlabel(\"log(C)\")\n",
    "plt.ylabel(\"Validation Log Loss\")\n",
    "plt.title(\"Validation Log Loss vs log(C)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895e2f98-513f-434c-97fa-becf5695d102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot validation AUC versus log(C)\n",
    "plt.figure()\n",
    "plt.plot(results[\"log_C\"], results[\"val_auc\"], marker=\"o\")\n",
    "plt.xlabel(\"log(C)\")\n",
    "plt.ylabel(\"Validation AUC\")\n",
    "plt.title(\"Validation AUC vs log(C)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0846a9f2-d752-4bfa-bb34-c314c6362757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best C using validation set\n",
    "best_idx = np.argmin(results[\"val_log_loss\"])\n",
    "C_star = results.loc[best_idx, \"C\"]\n",
    "best_log_loss = results.loc[best_idx, \"val_log_loss\"]\n",
    "best_auc = results.loc[best_idx, \"val_auc\"]\n",
    "\n",
    "print(\"Selected C*:\", C_star)\n",
    "print(\"Validation log loss at C*:\", best_log_loss)\n",
    "print(\"Validation AUC at C*:\", best_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ccb3e6-830f-4290-9cd3-4cd5c8177057",
   "metadata": {},
   "source": [
    "## Part 4: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b096e997-c2d8-46fb-9891-9d9d48c8ea85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refit using optimal C* from Part 3\n",
    "final_model = Pipeline([\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"logreg\", LogisticRegression(\n",
    "        C=C_star,\n",
    "        solver=\"lbfgs\",\n",
    "        max_iter=1000,\n",
    "        random_state=random_seed\n",
    "    ))\n",
    "])\n",
    "\n",
    "# TODO: Produce the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1b9850-0c11-48e2-a698-63ee3cdb20ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted probabilities for positive class\n",
    "y_test_prob = final_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# predicted labels at threshold 0.5\n",
    "y_test_pred = (y_test_prob >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654c6b54-adfd-4c50-82a2-4d0404a146b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "print(\"Confusion matrix (threshold=0.5):\")\n",
    "print(cm)\n",
    "\n",
    "# accuracy\n",
    "acc = accuracy_score(y_test, y_test_pred)\n",
    "print(\"\\nAccuracy:\", acc)\n",
    "\n",
    "# AUC\n",
    "auc = roc_auc_score(y_test, y_test_prob)\n",
    "print(\"Test set AUC:\", auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3132a8a0-db79-4dd0-8c48-b0adb4b2b77c",
   "metadata": {},
   "source": [
    "`sklearn`'s format:\n",
    "\n",
    "|                | Predicted 0 | Predicted 1 |\n",
    "|----------------|------------|------------|\n",
    "| True 0 (negative) | TN = 29   | FP = 31   |\n",
    "| True 1 (positive) | FN = 22   | TP = 118  |\n",
    "\n",
    "**Explanation of the entries:**\n",
    "\n",
    "- **TN = 29** â†’ correctly predicted negative cases (bad credit predicted as bad)  \n",
    "- **FP = 31** â†’ predicted positive but actually negative (bad credit predicted as good)  \n",
    "- **FN = 22** â†’ predicted negative but actually positive (good credit predicted as bad)  \n",
    "- **TP = 118** â†’ correctly predicted positive cases (good credit predicted as good)  \n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "- The model is **good at identifying positive cases** (TP = 118).  \n",
    "- There are still a number of **false positives** (FP = 31), which represent bad loans incorrectly predicted as good.  \n",
    "- Accuracy = (TP + TN) / total = (118 + 29) / (29 + 31 + 22 + 118) = 0.735.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9780b9e7-fea7-4db8-9d06-0f594dc6f9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC Curve\n",
    "RocCurveDisplay.from_estimator(final_model, X_test, y_test)\n",
    "plt.title(\"ROC Curve on Test Set\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ba800c-e0ce-4937-86d0-b33fbd9cf005",
   "metadata": {},
   "source": [
    "## Part 5: Threshold Choice and Economic Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a487cc3-90c6-4fed-a714-9f5263710f82",
   "metadata": {},
   "source": [
    "**Economic scenario: Credit approval decisions**\n",
    "\n",
    "The bank evaluates credit applications and must decide whether to approve each client. The outcomes are:\n",
    "\n",
    "| True credit risk | Action | Payoff |\n",
    "|-----------------|--------|--------|\n",
    "| Good (y=1)      | Approve | +100 (interest/profit) |\n",
    "| Bad (y=0)       | Approve | âˆ’50 (expected loss + admin) |\n",
    "| Good (y=1)      | Reject  | âˆ’20 (lost potential profit) |\n",
    "| Bad (y=0)       | Reject  | 0 |\n",
    "\n",
    "**Justification:**\n",
    "\n",
    "- Approving a good client generates profit (B > 0).  \n",
    "- Approving a bad client incurs loss (c > 0).  \n",
    "- Rejecting a good client has opportunity cost (L â‰¥ 0).  \n",
    "- Rejecting a bad client is neutral.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196c3c8e-af5a-4729-b0f6-438ebee3dc6f",
   "metadata": {},
   "source": [
    "### Q: Why is the below implementation problemetic? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facdc5bb-e124-47eb-9f8c-c48c447c8b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_prob = final_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "B, c, L = 100, 50, 20\n",
    "thresholds = np.linspace(0, 1, 101)\n",
    "expected_payoffs = []\n",
    "\n",
    "for t in thresholds:\n",
    "    y_pred = (y_val_prob >= t).astype(int)\n",
    "    TP = np.sum((y_val == 1) & (y_pred == 1))\n",
    "    FP = np.sum((y_val == 0) & (y_pred == 1))\n",
    "    FN = np.sum((y_val == 1) & (y_pred == 0))\n",
    "    expected_payoffs.append(TP*B - FP*c - FN*L)\n",
    "\n",
    "best_idx = np.argmax(expected_payoffs)\n",
    "t_star = thresholds[best_idx]\n",
    "max_payoff = expected_payoffs[best_idx]\n",
    "\n",
    "y_test_pred_opt = (final_model.predict_proba(X_test)[:, 1] >= t_star).astype(int)\n",
    "cm_opt = confusion_matrix(y_test, y_test_pred_opt)\n",
    "acc_opt = accuracy_score(y_test, y_test_pred_opt)\n",
    "\n",
    "print(\"Optimal threshold t*:\", t_star)\n",
    "print(\"Confusion matrix on test set at t*:\\n\", cm_opt)\n",
    "print(\"Maximum expected payoff on validation set:\", max_payoff)\n",
    "print(\"Accuracy on test set at t*:\", acc_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16113167-43c3-4acb-8985-2f359e7d5222",
   "metadata": {},
   "source": [
    "### Answer: Fitting and choosing t on validation set is cheating!\n",
    "\n",
    "The issue is that `final_model` was already **refit on the combined training + validation set**. By using it to compute `y_val_prob` on the validation set, we are effectively **peeking at the validation data** when choosing the threshold `t*`. This introduces **data leakage**, because the threshold is optimized using probabilities from a model that has already â€œseenâ€ the validation data. \n",
    "\n",
    "As a result:  \n",
    "\n",
    "- The chosen threshold `t*` may appear better than it would on truly unseen data.  \n",
    "- Test set performance may look optimistic, giving an **overestimation of accuracy or expected payoff**.  \n",
    "\n",
    "**Best practice:** Use a model trained **only on the training set** when evaluating validation probabilities to select `t*`. Then, after choosing `t*`, you can refit the final model on training + validation for deployment or test evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6db405-d005-48ea-9576-1bc40dbbd568",
   "metadata": {},
   "source": [
    "### The correct workflow of choosing the optimal threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965c29f9-7faf-414f-8197-470ea5e6df58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Train a model on training set only using optimal C\n",
    "model_train_only = Pipeline([\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"logreg\", LogisticRegression(\n",
    "        C=C_star,\n",
    "        solver=\"lbfgs\",\n",
    "        max_iter=1000,\n",
    "        random_state=random_seed\n",
    "    ))\n",
    "])\n",
    "\n",
    "model_train_only.fit(X_train, y_train)\n",
    "\n",
    "# Step 2: Compute predicted probabilities on validation set\n",
    "y_val_prob = model_train_only.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Step 3: Economic parameters (credit scenario)\n",
    "B, c, L = 100, 50, 20\n",
    "thresholds = np.linspace(0, 1, 101)\n",
    "expected_payoffs = []\n",
    "\n",
    "for t in thresholds:\n",
    "    y_pred = (y_val_prob >= t).astype(int)\n",
    "    TP = np.sum((y_val == 1) & (y_pred == 1))\n",
    "    FP = np.sum((y_val == 0) & (y_pred == 1))\n",
    "    FN = np.sum((y_val == 1) & (y_pred == 0))\n",
    "    expected_payoffs.append(TP*B - FP*c - FN*L)\n",
    "\n",
    "# Step 4: Choose threshold t* that maximizes expected payoff\n",
    "best_idx = np.argmax(expected_payoffs)\n",
    "t_star = thresholds[best_idx]\n",
    "max_payoff = expected_payoffs[best_idx]\n",
    "\n",
    "print(\"Optimal threshold t*:\", t_star)\n",
    "print(\"Maximum expected payoff on validation set:\", max_payoff)\n",
    "\n",
    "# Step 5: Refit final model on train + validation using optimal C\n",
    "final_model = Pipeline([\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"logreg\", LogisticRegression(\n",
    "        C=C_star,\n",
    "        solver=\"lbfgs\",\n",
    "        max_iter=1000,\n",
    "        random_state=random_seed\n",
    "    ))\n",
    "])\n",
    "X_trainval = pd.concat([X_train, X_val], axis=0)\n",
    "y_trainval = pd.concat([y_train, y_val], axis=0)\n",
    "final_model.fit(X_trainval, y_trainval)\n",
    "\n",
    "# Step 6: Evaluate on test set using t*\n",
    "y_test_pred_opt = (final_model.predict_proba(X_test)[:, 1] >= t_star).astype(int)\n",
    "cm_opt = confusion_matrix(y_test, y_test_pred_opt)\n",
    "acc_opt = accuracy_score(y_test, y_test_pred_opt)\n",
    "\n",
    "print(\"Confusion matrix on test set at t*:\\n\", cm_opt)\n",
    "print(\"Accuracy on test set at t*:\", acc_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2b800e-f513-4254-bc9a-0023ccff176e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
